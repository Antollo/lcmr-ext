{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lcmr.utils.presentation import display_img\n",
    "from lcmr_ext.dataset.dataset_7seg import Dataset7Seg\n",
    "from lcmr_ext.renderer.renderer2d import PyTorch3DRenderer2D\n",
    "from lcmr_ext.utils import collate_fn\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcmr.renderer.renderer2d import OpenGLRenderer2D\n",
    "from lcmr_ext.dataset.dataset_options import DatasetOptions\n",
    "from lcmr_ext.dataset.dataset_random import RandomDataset\n",
    "\n",
    "\n",
    "raster_size = (128, 128)\n",
    "dataset = RandomDataset(DatasetOptions(data_len=1024, scenes=True, Renderer=OpenGLRenderer2D))\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.detr.modeling_detr import DetrConfig\n",
    "\n",
    "from lcmr.reconstruction_model import ReconstructionModel\n",
    "from lcmr_ext.encoder import ResNet50Encoder\n",
    "from lcmr_ext.modeler import DETRModeler\n",
    "from lcmr.utils.matcher import Matcher\n",
    "\n",
    "renderer = PyTorch3DRenderer2D(raster_size, background_color=torch.tensor([0.0, 0.0, 0.0, 1.0]), device=device, with_alpha=False)\n",
    "encoder = ResNet50Encoder().to(device)\n",
    "modeler = DETRModeler(DetrConfig(num_queries=7)).to(device)\n",
    "\n",
    "model = ReconstructionModel(encoder, modeler, renderer)\n",
    "matcher = Matcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "epochs = 201\n",
    "lr = 0.0001\n",
    "show_step = 20\n",
    "\n",
    "optimizer = torch.optim.Adam(list(modeler.parameters()), lr=lr)\n",
    "\n",
    "def t_s_a_c(scene):\n",
    "    t = scene.layer.object.transformation.translation\n",
    "    s = scene.layer.object.transformation.scale\n",
    "    a = scene.layer.object.transformation.angle\n",
    "    c = scene.layer.object.appearance.color\n",
    "    return t, s, a, c\n",
    "\n",
    "batch_len = dataloader.batch_size\n",
    "layer_len = 1\n",
    "\n",
    "for epoch in (bar := tqdm(range(epochs))):\n",
    "    \n",
    "    for target_img, target_scene in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_img = target_img.to(device)\n",
    "        target_scene = target_scene.to(device)\n",
    "    \n",
    "        pred_scene, pred_img = model(target_img)\n",
    "        pred_img = pred_img[..., :3]\n",
    "        target_t, target_s, target_a, target_c = t_s_a_c(target_scene)\n",
    "        pred_t, pred_s, pred_a, pred_c = t_s_a_c(pred_scene)\n",
    "        \n",
    "            \n",
    "        ind_a, ind_b = matcher.match(target_t, pred_t)\n",
    "        target_t, target_s, target_a, target_c = matcher.gather(ind_a, (target_t, target_s, target_a, target_c))\n",
    "        pred_t, pred_s, pred_a, pred_c = matcher.gather(ind_b, (pred_t, pred_s, pred_a, pred_c))\n",
    "\n",
    "\n",
    "        loss = (target_t - pred_t).pow(2).mean()\n",
    "        #loss = loss + 0.05 * (target_s - pred_s).pow(2).mean()\n",
    "        # bezpośrednio na kącie nie bardzo jest sens, ale cos kąta pomiędzy ma sens\n",
    "        #loss = loss + 0.1 * (1 - torch.cos((target_a - pred_a) * 4)).mean() # Dowolne dopasowanie osi\n",
    "        \n",
    "        #cos_axis_aligned = torch.cos((target_a - pred_a) * 2).detach()\n",
    "        #loss = loss + 0.5 * ((1 - cos_axis_aligned) * (target_s - pred_s.flip(dims=[-1])).pow(2)).mean() # Dobry kąt, minimalizujemy złą skalę\n",
    "        #loss = loss + 0.5 * ((cos_axis_aligned + 1) * (target_s - pred_s).pow(2)).mean() # Zły kąt, minimalizujemy dobrą skalę\n",
    "        #loss = loss + (target_s - pred_s).pow(2).mean()\n",
    "        \n",
    "        loss = loss + 0.5 * (target_c - pred_c).pow(2).mean()\n",
    "        loss = loss + 0.1 * (target_s.mean(dim=-1) - pred_s.mean(dim=-1)).pow(2).mean()\n",
    "\n",
    "        loss = loss + (target_img - pred_img).pow(2).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bar.set_description(f\"loss: {loss.detach().cpu().item():.4f}\")\n",
    "            # might need to something.clamp_(0.0, 1.0)\n",
    "\n",
    "            if epoch % show_step == 0:\n",
    "                #print((1 - torch.cos((target_a - pred_a) * 2)).mean())\n",
    "                #print((target_s - pred_s.flip(dims=[-1])).pow(2).mean())\n",
    "                #print((target_s - pred_s).pow(2).mean())\n",
    "                #print(((1 - cos_axis_aligned) * (target_s - pred_s.flip(-1)).pow(2)).mean())\n",
    "                #print(((cos_axis_aligned + 1) * (target_s - pred_s).pow(2)).mean())\n",
    "                \n",
    "                display_img(pred_img[0])\n",
    "                display_img(target_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcmr_ext.dataset.dataset_7seg import random_image_7seg, font_file\n",
    "from PIL import ImageFont\n",
    "\n",
    "font_size = min(*raster_size) // 2\n",
    "font = ImageFont.truetype(font_file, font_size)\n",
    "            \n",
    "test_batch = torch.cat([random_image_7seg((0, 0, 0), 1, raster_size, font)[None, ...] for _ in range(16)], dim=0).to(device)\n",
    "#test_batch = torch.cat([dataset.data[i][0] for i in range(16)], dim=0).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred = model(test_batch)[1]\n",
    "\n",
    "display_img(torch.vstack([torch.hstack((t, p[..., :3])) for p, t in zip(pred, test_batch)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
